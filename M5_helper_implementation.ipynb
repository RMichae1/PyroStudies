{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M5_helper_implementation",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "117jp0bgapiSPQyLrG72WD5Fw2VAFOxyi",
      "authorship_tag": "ABX9TyOE6mhUVz41ZvJrGLk2meK0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RMichae1/PyroStudies/blob/master/M5_helper_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWmoG1gvRFqu",
        "colab_type": "code",
        "outputId": "0b6bbc56-c3e9-4ac2-f97f-a37f959cf95c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "!pip install pyro-ppl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyro-ppl in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.5.0+cu101)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.18.4)\n",
            "Requirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (3.2.1)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (4.41.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->pyro-ppl) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyoaOFYMS_2P",
        "colab_type": "code",
        "outputId": "298752fb-a65b-4767-d6e8-fcc1f2f42fbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import torch\n",
        "import pyro\n",
        "\n",
        "import pyro.distributions as dist\n",
        "from pyro.contrib.forecast import ForecastingModel\n",
        "from pyro.contrib.forecast import Forecaster\n",
        "from pyro.ops.tensor_utils import periodic_repeat\n",
        "from pyro.nn import PyroModule\n",
        "from pyro.nn import PyroParam\n",
        "import pyro.poutine as poutine\n",
        "from pyro.infer.reparam import LocScaleReparam, StableReparam\n",
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVRLfgacT0sV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.is_available()\n",
        "pyro.set_rng_seed(1001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e01hMbuIUJiM",
        "colab_type": "code",
        "outputId": "1845257d-d74e-4c02-ff73-dfbb9f220179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# set parameters for runs\n",
        "CUDA = torch.cuda.is_available()\n",
        "if CUDA:\n",
        "  torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "\n",
        "DRIVE = \"drive/My Drive/kaggle_m5/\"\n",
        "OUTPUT = os.path.join(DRIVE, \"results\")\n",
        "OUTFILE = os.path.join(OUTPUT, \"submission.csv\")\n",
        "\n",
        "if not os.path.exists(OUTPUT):\n",
        "    os.makedirs(OUTPUT)\n",
        "\n",
        "print(OUTFILE)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive/My Drive/kaggle_m5/results/submission.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxbTozPEcwIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### UTILITY TO LOAD M5 DATA\n",
        "import math\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "\n",
        "class M5Data:\n",
        "    \"\"\"\n",
        "    A helper class to read M5 source files and create submissions.\n",
        "    :param str data_path: Path to the folder that contains M5 data files, which is\n",
        "        either a single `.zip` file or some `.csv` files extracted from that zip file.\n",
        "    \"\"\"\n",
        "    num_states = 3\n",
        "    num_stores = 10\n",
        "    num_cats = 3\n",
        "    num_depts = 7\n",
        "    num_items = 3049\n",
        "    num_stores_by_state = [4, 3, 3]\n",
        "    num_depts_by_cat = [2, 2, 3]\n",
        "    num_items_by_cat = [565, 1047, 1437]\n",
        "    num_items_by_dept = [416, 149, 532, 515, 216, 398, 823]\n",
        "    num_timeseries = 30490  # store x item\n",
        "    num_aggregations = 42840\n",
        "    num_aggregations_by_level = [1, 3, 10, 3, 7, 9, 21, 30, 70, 3049, 9147, 30490]\n",
        "    num_quantiles = 9\n",
        "    quantiles = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]\n",
        "    aggregation_levels = [[],\n",
        "                          [\"state_id\"],\n",
        "                          [\"store_id\"],\n",
        "                          [\"cat_id\"],\n",
        "                          [\"dept_id\"],\n",
        "                          [\"state_id\", \"cat_id\"],\n",
        "                          [\"state_id\", \"dept_id\"],\n",
        "                          [\"store_id\", \"cat_id\"],\n",
        "                          [\"store_id\", \"dept_id\"],\n",
        "                          [\"item_id\"],\n",
        "                          [\"state_id\", \"item_id\"],\n",
        "                          [\"store_id\", \"item_id\"]]\n",
        "    event_types = [\"Cultural\", \"National\", \"Religious\", \"Sporting\"]\n",
        "\n",
        "    def __init__(self, data_path=None):\n",
        "        self.data_path = os.path.abspath(\"data\") if data_path is None else data_path\n",
        "        if not os.path.exists(self.data_path):\n",
        "            raise FileNotFoundError(f\"There is no folder '{self.data_path}'.\")\n",
        "\n",
        "        acc_path = os.path.join(self.data_path, \"m5-forecasting-accuracy.zip\")\n",
        "        unc_path = os.path.join(self.data_path, \"m5-forecasting-uncertainty.zip\")\n",
        "        self.acc_zipfile = zipfile.ZipFile(acc_path) if os.path.exists(acc_path) else None\n",
        "        self.unc_zipfile = zipfile.ZipFile(unc_path) if os.path.exists(unc_path) else None\n",
        "\n",
        "        self._sales_df = None\n",
        "        self._calendar_df = None\n",
        "        self._prices_df = None\n",
        "\n",
        "    @property\n",
        "    def num_days(self):\n",
        "        return self.calendar_df.shape[0]\n",
        "\n",
        "    @property\n",
        "    def num_train_days(self):\n",
        "        return self.sales_df.shape[1] - 5\n",
        "\n",
        "    @property\n",
        "    def sales_df(self):\n",
        "        if self._sales_df is None:\n",
        "            self._sales_df = self._read_csv(\"sales_train_validation.csv\", index_col=0)\n",
        "        return self._sales_df\n",
        "\n",
        "    @property\n",
        "    def calendar_df(self):\n",
        "        if self._calendar_df is None:\n",
        "            self._calendar_df = self._read_csv(\"calendar.csv\", index_col=0)\n",
        "        return self._calendar_df\n",
        "\n",
        "    @property\n",
        "    def prices_df(self):\n",
        "        if self._prices_df is None:\n",
        "            df = self._read_csv(\"sell_prices.csv\")\n",
        "            df[\"id\"] = df.item_id + \"_\" + df.store_id + \"_validation\"\n",
        "            df = pd.pivot_table(df, values=\"sell_price\", index=\"id\", columns=\"wm_yr_wk\")\n",
        "            self._prices_df = df.fillna(float('nan')).loc[self.sales_df.index]\n",
        "        return self._prices_df\n",
        "\n",
        "    def listdir(self):\n",
        "        \"\"\"\n",
        "        List all files in `self.data_path` folder.\n",
        "        \"\"\"\n",
        "        files = set(os.listdir(self.data_path))\n",
        "        if self.acc_zipfile:\n",
        "            files |= set(self.acc_zipfile.namelist())\n",
        "        if self.unc_zipfile:\n",
        "            files |= set(self.unc_zipfile.namelist())\n",
        "        return files\n",
        "\n",
        "    def _read_csv(self, filename, index_col=None, use_acc_file=True):\n",
        "        \"\"\"\n",
        "        Returns the dataframe from csv file ``filename``.\n",
        "        :param str filename: name of the file with trailing `.csv`.\n",
        "        :param int index_col: indicates which column from csv file is considered as index.\n",
        "        :param bool acc_file: whether to load data from accuracy.zip file or uncertainty.zip file.\n",
        "        \"\"\"\n",
        "        assert filename.endswith(\".csv\")\n",
        "        if filename not in self.listdir():\n",
        "            raise FileNotFoundError(f\"Cannot find either '{filename}' \"\n",
        "                                    \"or 'm5-forecasting-*.zip' file \"\n",
        "                                    f\"in '{self.data_path}'.\")\n",
        "\n",
        "        if use_acc_file and self.acc_zipfile and filename in self.acc_zipfile.namelist():\n",
        "            return pd.read_csv(self.acc_zipfile.open(filename), index_col=index_col)\n",
        "\n",
        "        if self.unc_zipfile and filename in self.unc_zipfile.namelist():\n",
        "            return pd.read_csv(self.unc_zipfile.open(filename), index_col=index_col)\n",
        "\n",
        "        return pd.read_csv(os.path.join(self.data_path, filename), index_col=index_col)\n",
        "\n",
        "    def get_sales(self):\n",
        "        \"\"\"\n",
        "        Returns `sales` torch.Tensor with shape `num_timeseries x num_train_days`.\n",
        "        \"\"\"\n",
        "        return torch.from_numpy(self.sales_df.iloc[:, 5:].values).type(torch.get_default_dtype())\n",
        "\n",
        "    def get_prices(self, fillna=0.):\n",
        "        \"\"\"\n",
        "        Returns `prices` torch.Tensor with shape `num_timeseries x num_days`.\n",
        "        In some days, there are some items not available, so their prices will be NaN.\n",
        "        :param float fillna: a float value to replace NaN. Defaults to 0.\n",
        "        \"\"\"\n",
        "        x = torch.from_numpy(self.prices_df.values).type(torch.get_default_dtype())\n",
        "        x[torch.isnan(x)] = fillna\n",
        "        x = x.repeat_interleave(7, dim=-1)[:, :self.calendar_df.shape[0]]\n",
        "        assert x.shape == (self.num_timeseries, self.num_days)\n",
        "        return x\n",
        "\n",
        "    def get_snap(self):\n",
        "        \"\"\"\n",
        "        Returns a `num_days x 3` boolean tensor which indicates whether\n",
        "        SNAP purchases are allowed at a state in a particular day. The order\n",
        "        of the first dimension indicates the states \"CA\", \"TX\", \"WI\" respectively.\n",
        "        Usage::\n",
        "            >>> m5 = M5Data()\n",
        "            >>> snap = m5.get_snap()\n",
        "            >>> assert snap.shape == (m5.num_states, m5.num_days)\n",
        "            >>> snap = snap.repeat_interleave(torch.tensor(m5.num_stores_by_state), dim=0)\n",
        "            >>> assert snap.shape == (m5.num_stores, m5.num_days)\n",
        "        \"\"\"\n",
        "        snap = self.calendar_df[[\"snap_CA\", \"snap_TX\", \"snap_WI\"]].values\n",
        "        x = torch.from_numpy(snap).type(torch.get_default_dtype())\n",
        "        assert x.shape == (self.num_days, 3)\n",
        "        return x\n",
        "\n",
        "    def get_event(self, by_types=False):\n",
        "        \"\"\"\n",
        "        Returns a tensor with length `num_days` indicating whether there are\n",
        "        special events on a particular day.\n",
        "        There are 4 types of events: \"Cultural\", \"National\", \"Religious\", \"Sporting\".\n",
        "        :param bool by_types: if True, returns a `num_days x 4` tensor indicating\n",
        "            special event by type. Otherwise, only returns a `num_days x 1` tensor indicating\n",
        "            whether there is a special event.\n",
        "        \"\"\"\n",
        "        if not by_types:\n",
        "            event = self.calendar_df[\"event_type_1\"].notnull().values[..., None]\n",
        "            x = torch.from_numpy(event).type(torch.get_default_dtype())\n",
        "            assert x.shape == (self.num_days, 1)\n",
        "            return x\n",
        "\n",
        "        types = self.event_types\n",
        "        event1 = pd.get_dummies(self.calendar_df[\"event_type_1\"])[types].astype(bool)\n",
        "        event2 = pd.DataFrame(columns=types)\n",
        "        types2 = [\"Cultural\", \"Religious\"]\n",
        "        event2[types2] = pd.get_dummies(self.calendar_df[\"event_type_2\"])[types2].astype(bool)\n",
        "        event2.fillna(False, inplace=True)\n",
        "        x = torch.from_numpy(event1.values | event2.values).type(torch.get_default_dtype())\n",
        "        assert x.shape == (self.num_days, 4)\n",
        "        return x\n",
        "\n",
        "    def get_dummy_day_of_month(self):\n",
        "        \"\"\"\n",
        "        Returns dummy day of month tensor with shape `num_days x 31`.\n",
        "        \"\"\"\n",
        "        dom = pd.get_dummies(pd.to_datetime(self.calendar_df.index).day).values\n",
        "        x = torch.from_numpy(dom).type(torch.get_default_dtype())\n",
        "        assert x.shape == (self.num_days, 31)\n",
        "        return x\n",
        "\n",
        "    def get_dummy_month_of_year(self):\n",
        "        \"\"\"\n",
        "        Returns dummy month of year tensor with shape `num_days x 12`.\n",
        "        \"\"\"\n",
        "        moy = pd.get_dummies(pd.to_datetime(self.calendar_df.index).month).values\n",
        "        x = torch.from_numpy(moy).type(torch.get_default_dtype())\n",
        "        assert x.shape == (self.num_days, 12)\n",
        "        return x\n",
        "\n",
        "    def get_dummy_day_of_week(self):\n",
        "        \"\"\"\n",
        "        Returns dummy day of week tensor with shape `num_days x 7`.\n",
        "        \"\"\"\n",
        "        dow = pd.get_dummies(self.calendar_df.wday).values\n",
        "        x = torch.from_numpy(dow).type(torch.get_default_dtype())\n",
        "        assert x.shape == (self.num_days, 7)\n",
        "        return x\n",
        "\n",
        "    def get_dummy_year(self):\n",
        "        \"\"\"\n",
        "        Returns dummy year tensor with shape `num_days x 6`.\n",
        "        \"\"\"\n",
        "        year = pd.get_dummies(pd.to_datetime(self.calendar_df.index).year).values\n",
        "        x = torch.from_numpy(year).type(torch.get_default_dtype())\n",
        "        assert x.shape == (self.num_days, 6)\n",
        "        return x\n",
        "\n",
        "    def get_christmas(self):\n",
        "        \"\"\"\n",
        "        Returns a boolean 2D tensor with shape `num_days x 1` indicating\n",
        "        if that day is Chrismas.\n",
        "        \"\"\"\n",
        "        christmas = self.calendar_df.index.str.endswith(\"12-25\")[..., None]\n",
        "        x = torch.from_numpy(christmas).type(torch.get_default_dtype())\n",
        "        assert x.shape == (self.num_days, 1)\n",
        "        return x\n",
        "\n",
        "    def get_dummy_state(self):\n",
        "        \"\"\"\n",
        "        Returns dummy state tensor with shape `num_timeseries x num_states`.\n",
        "        \"\"\"\n",
        "        state = pd.get_dummies(self.sales_df.state_id)[self.sales_df.state_id.unique()].values\n",
        "        x = torch.from_numpy(state).type(torch.get_default_dtype())\n",
        "        assert x.shape == (self.num_timeseries, self.num_states)\n",
        "        return x\n",
        "\n",
        "    def get_dummy_cat(self):\n",
        "        \"\"\"\n",
        "        Returns dummy cat tensor with shape `num_timeseries x num_states`.\n",
        "        \"\"\"\n",
        "        cat = pd.get_dummies(self.sales_df.cat_id)[self.sales_df.cat_id.unique()].values\n",
        "        x = torch.from_numpy(cat).type(torch.get_default_dtype())\n",
        "        assert x.shape == (self.num_timeseries, self.num_cats)\n",
        "        return x\n",
        "\n",
        "    def get_dummy_dept(self):\n",
        "        \"\"\"\n",
        "        Returns dummy dept tensor with shape `num_timeseries x num_states`.\n",
        "        \"\"\"\n",
        "        dept = pd.get_dummies(self.sales_df.dept_id)[self.sales_df.dept_id.unique()].values\n",
        "        x = torch.from_numpy(dept).type(torch.get_default_dtype())\n",
        "        assert x.shape == (self.num_timeseries, self.num_depts)\n",
        "        return x\n",
        "\n",
        "    def get_aggregated_sales(self, level):\n",
        "        \"\"\"\n",
        "        Returns aggregated sales at a particular aggregation level.\n",
        "        The result will be a tensor with shape `num_timeseries x num_train_days`.\n",
        "        \"\"\"\n",
        "        if level == self.aggregation_levels[-1]:\n",
        "            x = self.sales_df.iloc[:, 5:].values\n",
        "        elif level == self.aggregation_levels[0]:\n",
        "            x = self.sales_df.iloc[:, 5:].sum().values[None, :]\n",
        "        else:\n",
        "            df = self.sales_df.groupby(level, sort=False).sum()\n",
        "            x = df.values\n",
        "\n",
        "        return torch.from_numpy(x).type(torch.get_default_dtype())\n",
        "\n",
        "    def get_aggregated_ma_dollar_sales(self, level):\n",
        "        \"\"\"\n",
        "        Returns aggregated \"moving average\" dollar sales at a particular aggregation level\n",
        "        during the last 28 days.\n",
        "        The result can be used as `weight` for evaluation metrics.\n",
        "        \"\"\"\n",
        "        prices = self.prices_df.fillna(0.).values.repeat(7, axis=1)[:, :self.sales_df.shape[1] - 5]\n",
        "        df = (self.sales_df.iloc[:, 5:] * prices).T.rolling(28, min_periods=1).mean().T\n",
        "\n",
        "        if level == self.aggregation_levels[-1]:\n",
        "            x = df.values\n",
        "        elif level == self.aggregation_levels[0]:\n",
        "            x = df.sum().values[None, :]\n",
        "        else:\n",
        "            for g in level:\n",
        "                df[g] = self.sales_df[g]\n",
        "\n",
        "            df = df.groupby(level, sort=False).sum()\n",
        "            x = df.values\n",
        "\n",
        "        return torch.from_numpy(x).type(torch.get_default_dtype())\n",
        "\n",
        "    def get_all_aggregated_sales(self):\n",
        "        \"\"\"\n",
        "        Returns aggregated sales for all aggregation levels.\n",
        "        \"\"\"\n",
        "        xs = []\n",
        "        for level in self.aggregation_levels:\n",
        "            xs.append(self.get_aggregated_sales(level))\n",
        "        xs = torch.cat(xs, 0)\n",
        "        assert xs.shape[0] == self.num_aggregations\n",
        "        return xs\n",
        "\n",
        "    def get_all_aggregated_ma_dollar_sales(self):\n",
        "        \"\"\"\n",
        "        Returns aggregated \"moving average\" dollar sales for all aggregation levels.\n",
        "        \"\"\"\n",
        "        xs = []\n",
        "        for level in self.aggregation_levels:\n",
        "            xs.append(self.get_aggregated_ma_dollar_sales(level))\n",
        "        xs = torch.cat(xs, 0)\n",
        "        assert xs.shape[0] == self.num_aggregations\n",
        "        return xs\n",
        "\n",
        "    def aggregate_samples(self, samples, level, *extra_levels):\n",
        "        \"\"\"\n",
        "        Aggregates samples (at the lowest level) to a specific level.\n",
        "        Usage::\n",
        "            >>> m5 = M5Data()\n",
        "            >>> o = []\n",
        "            >>> for level in m5.aggregation_levels:\n",
        "            ...     print(\"Level\", level)\n",
        "            ...     o.append(m5.aggregate_samples(samples, level))\n",
        "            >>> o = torch.cat(o, 1)\n",
        "            >>> q = np.quantile(o.numpy(), m5.quantiles, axis=0)  # compute quantiles\n",
        "            >>> m5.make_uncertainty_submission(\"foo.csv\", q)\n",
        "        :param torch.Tensor samples: a tensor with shape `num_samples x num_timeseries x num_days`\n",
        "        :param list level: which level to aggregate\n",
        "        :param extra_levels: additional levels to aggregate; the results for all levels will be\n",
        "            concatenated together.\n",
        "        :returns: a tensor with shape `num_samples x num_aggregated_timeseries x num_days`.\n",
        "        \"\"\"\n",
        "        assert torch.is_tensor(samples)\n",
        "        assert samples.dim() == 3\n",
        "        assert samples.size(1) == self.num_timeseries\n",
        "        num_samples, duration = samples.size(0), samples.size(-1)\n",
        "        x = samples.reshape(num_samples, self.num_stores, self.num_items, duration)\n",
        "\n",
        "        if \"state_id\" in level:\n",
        "            tmp = []\n",
        "            pos = 0\n",
        "            for n in self.num_stores_by_state:\n",
        "                tmp.append(x[:, pos:pos + n].sum(1, keepdim=True))\n",
        "                pos = pos + n\n",
        "            x = torch.cat(tmp, dim=1)\n",
        "        elif \"store_id\" in level:\n",
        "            pass\n",
        "        else:\n",
        "            x = x.sum(1, keepdim=True)\n",
        "\n",
        "        if \"cat_id\" in level:\n",
        "            tmp = []\n",
        "            pos = 0\n",
        "            for n in self.num_items_by_cat:\n",
        "                tmp.append(x[:, :, pos:pos + n].sum(2, keepdim=True))\n",
        "                pos = pos + n\n",
        "            x = torch.cat(tmp, dim=2)\n",
        "        elif \"dept_id\" in level:\n",
        "            tmp = []\n",
        "            pos = 0\n",
        "            for n in self.num_items_by_dept:\n",
        "                tmp.append(x[:, :, pos:pos + n].sum(2, keepdim=True))\n",
        "                pos = pos + n\n",
        "            x = torch.cat(tmp, dim=2)\n",
        "        elif \"item_id\" in level:\n",
        "            pass\n",
        "        else:\n",
        "            x = x.sum(2, keepdim=True)\n",
        "\n",
        "        n = self.num_aggregations_by_level[self.aggregation_levels.index(level)]\n",
        "        x = x.reshape(num_samples, n, duration)\n",
        "        if extra_levels:\n",
        "            tmp = [x]\n",
        "            for level in extra_levels:\n",
        "                tmp.append(self.aggregate_samples(samples, level))\n",
        "            x = torch.cat(tmp, 1)\n",
        "        return x\n",
        "\n",
        "    def make_accuracy_submission(self, filename, prediction):\n",
        "        \"\"\"\n",
        "        Makes submission file given prediction result.\n",
        "        :param str filename: name of the submission file.\n",
        "        :param torch.Tensor predicition: the prediction tensor with shape `num_timeseries x 28`.\n",
        "        \"\"\"\n",
        "        df = self._read_csv(\"sample_submission.csv\", index_col=0)\n",
        "        if torch.is_tensor(prediction):\n",
        "            prediction = prediction.detach().cpu().numpy()\n",
        "        assert isinstance(prediction, np.ndarray)\n",
        "        assert prediction.shape == (self.num_timeseries, 28)\n",
        "        # the later 28 days only available 1 month before the deadline\n",
        "        assert df.shape[0] == prediction.shape[0] * 2\n",
        "        df.iloc[:prediction.shape[0], :] = prediction\n",
        "        df.to_csv(filename)\n",
        "\n",
        "    def make_uncertainty_submission(self, filename, prediction, float_format='%.3g'):\n",
        "        \"\"\"\n",
        "        Makes submission file given prediction result.\n",
        "        :param str filename: name of the submission file.\n",
        "        :param torch.Tensor predicition: the prediction tensor with shape\n",
        "            `9 x num_aggregations x 28`. The first dimension indicates\n",
        "            9 quantiles defined in `self.quantiles`. The second dimension\n",
        "            indicates aggreated series defined in `self.aggregation_levels`,\n",
        "            with corresponding order. This is also the order of\n",
        "            submission file.\n",
        "        \"\"\"\n",
        "        df = self._read_csv(\"sample_submission.csv\", index_col=0, use_acc_file=False)\n",
        "        if torch.is_tensor(prediction):\n",
        "            prediction = prediction.detach().cpu().numpy()\n",
        "        assert isinstance(prediction, np.ndarray)\n",
        "        assert prediction.shape == (9, self.num_aggregations, 28)\n",
        "\n",
        "        # correct the messy index in submission file\n",
        "        tmp = []\n",
        "        pos = 0\n",
        "        for level, n in zip(self.aggregation_levels, self.num_aggregations_by_level):\n",
        "            if level == self.aggregation_levels[0] or level == self.aggregation_levels[-1]:\n",
        "                tmp.append(prediction[:, pos:pos+n])\n",
        "            else:\n",
        "                tmp_df = self.sales_df.groupby(level, sort=False)[[\"item_id\"]].count()\n",
        "                tmp_df[\"id\"] = range(tmp_df.shape[0])\n",
        "                tmp_df = tmp_df.sort_index()\n",
        "                if level == self.aggregation_levels[-2]:\n",
        "                    tmp_df = tmp_df.reindex([\"WI\", \"CA\", \"TX\"], level=0)\n",
        "                new_index = tmp_df[\"id\"].values\n",
        "                tmp.append(prediction[:, pos:pos+n][:, new_index])\n",
        "            pos = pos + n\n",
        "        prediction = np.concatenate(tmp, axis=1)\n",
        "\n",
        "        prediction = prediction.reshape(-1, 28)\n",
        "        # the later 28 days only available 1 month before the deadline\n",
        "        assert df.shape[0] == prediction.shape[0] * 2\n",
        "        df.iloc[:prediction.shape[0], :] = prediction\n",
        "        # use float_format to reduce the size of output file,\n",
        "        # recommended at https://www.kaggle.com/c/m5-forecasting-uncertainty/discussion/135049\n",
        "        df.to_csv(filename, float_format=float_format)\n",
        "\n",
        "\n",
        "class BatchDataLoader:\n",
        "    \"\"\"\n",
        "    DataLoader class which iterates over the dataset (data_x, data_y) in batch.\n",
        "    Usage::\n",
        "        >>> data_loader = BatchDataLoader(data_x, data_y, batch_size=1000)\n",
        "        >>> for batch_x, batch_y in data_loader:\n",
        "        ...     # do something with batch_x, batch_y\n",
        "    \"\"\"\n",
        "    def __init__(self, data_x, data_y, batch_size, shuffle=True):\n",
        "        super().__init__()\n",
        "        self.data_x = data_x\n",
        "        self.data_y = data_y\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        assert self.data_x.size(0) == self.data_y.size(0)\n",
        "        assert len(self) > 0\n",
        "\n",
        "    @property\n",
        "    def size(self):\n",
        "        return self.data_x.size(0)\n",
        "\n",
        "    def __len__(self):\n",
        "        # XXX: should we remove or include the tailing data (which has len < batch_size)?\n",
        "        return math.ceil(self.size / self.batch_size)\n",
        "\n",
        "    def _sample_batch_indices(self):\n",
        "        if self.shuffle:\n",
        "            idx = torch.randperm(self.size)\n",
        "        else:\n",
        "            idx = torch.arange(self.size)\n",
        "        return idx, len(self)\n",
        "\n",
        "    def __iter__(self):\n",
        "        idx, n_batches = self._sample_batch_indices()\n",
        "        for i in range(n_batches):\n",
        "            _slice = idx[i * self.batch_size: (i + 1) * self.batch_size]\n",
        "            yield self.data_x[_slice], self.data_y[_slice]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-5kkJ-wY1W1",
        "colab_type": "text"
      },
      "source": [
        "**See Model 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vom5taGiZSTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TopDownModel(ForecastingModel):\n",
        "  \"\"\"\n",
        "  Forecasting Model 1\n",
        "  \"\"\"\n",
        "  def model(self, zero_data, covariates):\n",
        "    # check univariate data\n",
        "    assert zero_data.size(-1) == 1 \n",
        "    duration = zero_data.size(-2)\n",
        "\n",
        "    time, feature = covariates[..., 0], covariates[..., 1:]\n",
        "\n",
        "    bias = pyro.sample(\"bias\", dist.Normal(0, 10))\n",
        "    trend_coef = pyro.sample(\"trend\", dist.LogNormal(-2, 1))\n",
        "    trend = trend_coef * time\n",
        "\n",
        "    weight = pyro.sample(\"weight\", dist.Normal(0, 1).expand(\n",
        "        [feature.size(-1)]).to_event(1))\n",
        "    regressor = (weight * feature).sum(-1)\n",
        "\n",
        "    # weekly seasonality as indpendent events\n",
        "    with pyro.plate(\"day_of_week\", 7, dim=-1):\n",
        "      seasonal = pyro.sample(\"seasonal\", dist.Normal(0, 5))\n",
        "    seasonal = periodic_repeat(seasonal, duration, dim=-1)\n",
        "\n",
        "    # predict\n",
        "    prediction = bias + trend + seasonal + regressor\n",
        "    # Pyro Forecast is multivariate - univariate timeseries is needed\n",
        "    prediction = prediction.unsqueeze(-1)\n",
        "\n",
        "    # heavy tail nose to account for outliers\n",
        "    stability = pyro.sample(\"noise_stability\", dist.Uniform(1, 2).expand([1]).to_event(1))\n",
        "    skew = pyro.sample(\"noise_skew\", dist.Uniform(-1, 1).expand([1]).to_event(1))\n",
        "    scale = pyro.sample(\"noise_scale\", dist.LogNormal(-5, 5).expand([1]).to_event(1))\n",
        "    noise_dist = dist.Stable(stability, skew, scale)\n",
        "    with poutine.reparam(config={\"residual\": StableReparam()}):\n",
        "      self.predict(noise_dist, prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_l_FDp6fR0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform(pred, truth):\n",
        "  \"\"\"\n",
        "  Helper function to undo transformation\n",
        "  \"\"\"\n",
        "  return pred.exp(), truth.exp()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN9cN3MhsQfd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bounded_exp(x, bound=1e3):\n",
        "  return (x-math.log(bound)).sigmoid() * bound"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlhAtoRi13t5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_agg(pred, truth):\n",
        "  num_samples, duration = pred.size(0), pred.size(-2)\n",
        "  pred = pred.reshape(num_sample, -1, duration)\n",
        "  truth = truth.round().reshape(-1, duration).cpu()\n",
        "  agg_pred = m5_data.aggregate_samples(pred, *m5_data.aggregation_levels)\n",
        "  agg_truth = m5_data.aggregate_samples(truth.unsqueeze(0),\n",
        "                                        *m5_data.aggregation_levels).squeeze(0)\n",
        "  return agg_pred.unsqueeze(-1), agg_truth.unsqueeze(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq0DFwkZ2oUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forecaster_opt_fn(t0=None, t1=None, t2=None):\n",
        "  forecaster_opt = {\n",
        "      \"create_plates\": create_plates,\n",
        "      \"learning_rate\": 0.1,\n",
        "      \"learning_rate_decay\": 0.1,\n",
        "      \"clip_norm\": 10.,\n",
        "      \"num_steps\": 1001,\n",
        "      \"log_every\": 100,\n",
        "      \"guide\": NormalGuide(create_plates),\n",
        "  }\n",
        "  return forecaster_opt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDCXR5RbcgRw",
        "colab_type": "code",
        "outputId": "9ebcc6de-e50c-4aa0-c74d-cbc83bcb2436",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "m5_data = M5Data(data_path=DRIVE)\n",
        "data = m5_data.get_aggregated_sales(m5_data.aggregation_levels[0])[0].unsqueeze(-1)\n",
        "\n",
        "print(\"Tensor Shape aggregated data: {}\".format(data.shape))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor Shape aggregated data: torch.Size([1913, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLfr47rKdca4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# log transform the sh*t out of it\n",
        "data = data.log()\n",
        "\n",
        "T0 = 0\n",
        "T2 = data.size(-2) + 28 # end + submission\n",
        "time = torch.arange(T0, float(T2), device=\"cpu\") / 365\n",
        "covariates = torch.cat([\n",
        "                        time.unsqueeze(-1),\n",
        "                        # dummy months as features\n",
        "                        m5_data.get_dummy_day_of_month()[T0:T2],], dim=-1)\n",
        "\n",
        "if CUDA:\n",
        "  data = data.cuda()\n",
        "  covariates = covariates.cuda()\n",
        "  torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "\n",
        "# forecaster_opt = {\n",
        "#     \"learning_rate\": 0.045,\n",
        "#     \"learning_rate_decay\": 0.09,\n",
        "#     \"clip_norm\": 10,\n",
        "#     \"num_steps\": 1001,\n",
        "#     \"log_every\": 100,\n",
        "# }\n",
        "\n",
        "forecaster_opt = {\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"learning_rate_decay\": 0.08,\n",
        "    \"clip_norm\": 5,\n",
        "    \"num_steps\": 1001,\n",
        "    \"log_every\": 100,\n",
        "}\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iVhTGgbftuf",
        "colab_type": "code",
        "outputId": "22280381-cc27-4c94-9800-0872465dacac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# do the forecasting and prediction\n",
        "forecaster = Forecaster(TopDownModel(), data, covariates[:-28], **forecaster_opt)\n",
        "samples = forecaster(data, covariates, num_samples=1000).exp().squeeze(-1).cpu()\n",
        "pred = samples.mean(0)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n",
            "INFO \t step    0 loss = 4.86266e+06\n",
            "INFO \t step  100 loss = 10.9222\n",
            "INFO \t step  200 loss = 4.59061\n",
            "INFO \t step  300 loss = 1.42875\n",
            "INFO \t step  400 loss = 0.0327811\n",
            "INFO \t step  500 loss = 0.0363195\n",
            "INFO \t step  600 loss = -0.229535\n",
            "INFO \t step  700 loss = -0.244174\n",
            "INFO \t step  800 loss = -0.262966\n",
            "INFO \t step  900 loss = -0.273359\n",
            "INFO \t step 1000 loss = -0.265945\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk625CU9jXtT",
        "colab_type": "code",
        "outputId": "3cde41f9-eb9b-42b7-a2b1-a5a3be969190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(OUTFILE)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive/My Drive/kaggle_m5/results/submission.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KXLW2s0i1bM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate accuracy and uncertainty\n",
        "\n",
        "# top down distributed aggregated forecast sales `pred`\n",
        "sales_last28 = m5_data.get_aggregated_sales(\n",
        "    m5_data.aggregation_levels[-1])[:, -28:]\n",
        "proportion = sales_last28.sum(-1) / sales_last28.sum()\n",
        "prediction = proportion.ger(pred)\n",
        "\n",
        "print\n",
        "m5_data.make_accuracy_submission(OUTFILE, \n",
        "                                 prediction)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCtqtPFUhyHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# use top down for uncertainty prediction\n",
        "non_agg_samples = samples.unsqueeze(1) * proportion.unsqueeze(-1)\n",
        "# apply poisson distribution to account for low std and quantile values due to aggregate\n",
        "non_agg_samples = torch.poisson(non_agg_samples)\n",
        "agg_samples = m5_data.aggregate_samples(non_agg_samples, \n",
        "                                        *m5_data.aggregation_levels)\n",
        "print(\"Calculate quantiles...\")\n",
        "q = np.quantile(agg_samples.numpy(), m5_data.quantiles, axis=0)\n",
        "print(\"Make Uncertainty Submission...\")\n",
        "filename, ext = os.path.splitext(OUTFILE)\n",
        "m5_data.make_uncertainty_submission(OUTFILE + \"_uncertainty\" + ext, q, \n",
        "                                    float_format=\"%.3f\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQgXptSbjGo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Model2 hierarchical \n",
        "class HierarchyModel(ForecastingModel):\n",
        "  def __init__(self, snap, dept, saled, log_ma):\n",
        "    \"\"\"\n",
        "    Store covariates in the constructor, reshaping is expensive\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    # boolean for indicating SNAP\n",
        "    assert snap.shape == (10, 1, snap.size(2), 1)\n",
        "    self.snap = snap\n",
        "    # one-hot encoding for department of each product\n",
        "    assert dept.shape == (10, 3049, 1, 7)\n",
        "    self.dept = dept\n",
        "    # product availability\n",
        "    assert saled.shape == (10, 3049, snap.size(2), 3)\n",
        "    self.saled = saled\n",
        "    # moving average features - covariate as time-local feature\n",
        "    assert log_ma.shape == (10, 3049, log_ma.size(2), 3)\n",
        "    self.log_ma = log_ma\n",
        "\n",
        "  def model(self, zero_data, covariates):\n",
        "    # univariate data\n",
        "    assert zero_data.size(-1) == 1 \n",
        "    time_index = covariates.squeeze(-1)\n",
        "\n",
        "    store_plate = pyro.plate(\"store\", num_stores, dim=-3)\n",
        "    product_plate = pyro.plate(\"product\", num_products, dim=-2)\n",
        "    day_of_week_plate = pyro.plate(\"day_of_week\", 7, dim=-1)\n",
        "\n",
        "    snap = self.snap[..., time_index, :]\n",
        "    # subsample data\n",
        "    with product_plate:\n",
        "      dept = pyro.subsample(self.dept, event_dim=1)\n",
        "      saled = pyro.subsample(self.saled, event_dim=1)[..., time_index, :]\n",
        "      log_ma = pyro.subsample(self.log_ma, event_dim=1)[..., time_index, :]\n",
        "\n",
        "    # latent variable for each store and dept\n",
        "    with store_plate:\n",
        "      ma_weight = pyro.sample(\"ma_weight\", \n",
        "                              dist.Normal(0, 1).expand([2, log_ma.size(-1), 7]).to_event(3))\n",
        "      ma_weight = ma_weight.matmul(dept.unsqueeze(-2).unsqueeze(-1)).squeeze(-1)\n",
        "      moving_average = ma_weight.matmul(log_ma.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "      snap_weight = pyro.sample(\"snap_weight\",\n",
        "                                dist.Normal(0, 1).expand([2, 7]).to_event(2))\n",
        "      snap_weight = snap_weight.matmul(dept.unsqueeze(-1)).squeeze(-1)\n",
        "      snap_effect = snap_weight * snap\n",
        "\n",
        "      with day_of_week_plate:\n",
        "        seasonal = pyro.sample(\"seasonal\", dist.Normal(0, 1).expand([2, 7]).to_event(2))\n",
        "      seasonal = seasonal.matmul(dept.unsqueeze(-1)).squeeze(-1)\n",
        "      seasonal = periodic_repeat(seasonal, duration, dim=-2)\n",
        "    \n",
        "    prediction = moving_average + snap_effect + seasonal\n",
        "    log_mean, log_scale = prediction[..., :1], prediction[..., 1:]\n",
        "    # add small bias 1e-3 to avoid mean=scale=0\n",
        "    mean = bounded_exp(log_mean) * saled + 1e-3\n",
        "    scale = bounded_exp(log_scale) * saled + 1e-3\n",
        "\n",
        "    rate = scale.reciprocal()\n",
        "    concentration = mean * rate\n",
        "    # alt GammaPoisson or NegativeBinomial\n",
        "    noise_dist = dist.Gamma(concentration, rate)\n",
        "\n",
        "    with store_plate, product_plate:\n",
        "      self.predict(noise_dist, mean.new_zeros(mean.shape))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZWau6iIwSMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NormalGuide(PyroModule):\n",
        "  \"\"\"\n",
        "  Custom guide as to not use the AutoNormal default guide \n",
        "  \"\"\"\n",
        "  def __init__(self, create_plates=None):\n",
        "    super().__init__()\n",
        "    # define shapes of sample sites\n",
        "    self.ma_weight_loc = PyroParam(torch.zeros(10, 1, 1, 2, 3, 7), event_dim=3)\n",
        "    self.ma_weight_scale = PyroParam(torch.ones(10, 1, 1, 2, 3, 7) * 0.1,\n",
        "                                     dist.constraints.positive, event_dim=3)\n",
        "    self.snap_weight_loc = PyroParam(torch.zeros(10, 1, 1, 2, 7), event_dim=2)\n",
        "    self.snap_weight_scale = PyroParam(torch.ones(10, 1, 1, 2, 7) * 0.1,\n",
        "                                       dist.constraints.positive, event_dim=2)\n",
        "    self.seasonal_loc = PyroParam(torch.zeros(10, 1, 7, 2, 7), event_dim=2)\n",
        "    self.seasonal_scale = PyroParam(torch.ones(10, 1, 7, 2, 7) * 0.1, \n",
        "                                    dist.constraints.positive, event_dim=2)\n",
        "    self.create_plates = create_plates\n",
        "\n",
        "  def forward(self, data, covariates):\n",
        "    num_stores = data.size(0)\n",
        "    if self.create_plates is not None:\n",
        "      product_plate = self.create_plates(data, covatiates)\n",
        "      store_plate = pyro.plate(\"store\", num_stores, dim=-3)\n",
        "      day_of_week_plate = pyro.plate(\"day_of_week\", 7, dim=-1)\n",
        "\n",
        "    with store_plate:\n",
        "      pyro.sample(\"ma_weight\", dist.Normal(self.ma_weight_loc, \n",
        "                                           self.ma_weight_scale).to_event(3))\n",
        "      pyro.sample(\"snap_weight\", dist.Normal(self.snap_weight_loc, \n",
        "                                             self.snap_weight_scale).to_event(2))\n",
        "      with day_of_week_plate:\n",
        "        pyro.sample(\"seasonal\", \n",
        "                    dist.Normal(self.seasonal_loc, self.seasonal_scale).to_event(2))\n",
        "\n",
        "def create_plates(zero_data, covariates):\n",
        "  return pyro.plate(\"product\", zero_data.shape[1], subsample_size=60, dim=-2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcF1fDw0yxrA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class M5Forecaster(Forecaster):\n",
        "  \"\"\"\n",
        "  Forecaster that draws subsamples and casts batches to CPU\n",
        "  will skip unnecessary training data\n",
        "  \"\"\"\n",
        "  def forward(self, data, covariates, num_samples, batch_size=None):\n",
        "    if batch_size is not None:\n",
        "      batches = []\n",
        "      while num_samples > 0:\n",
        "        batch = self.forward(data, covariates, min(num_samples, batch_size))\n",
        "        batches.append(batch)\n",
        "        num_samples -= batch_size\n",
        "      return torch.cat(batches)\n",
        "    # skip part that has no conflict with weekly seasonal patterns\n",
        "    skip = data.size(-2) // 7 * 7\n",
        "    return super().forward(data[..., skip:, :], covariates[skip:], num_samples).cpu()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7dDg1mczivr",
        "colab_type": "code",
        "outputId": "5487de2e-d70d-4fb0-abad-d0063c50f23d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "data = m5_data.get_aggregated_sales(m5_data.aggregation_levels[-1])\n",
        "data = data.reshape(10, 3049, -1, 1)\n",
        "\n",
        "T0 = 37 + 28*3 # skip small begin to calc moving average\n",
        "T2 = data.size(-2) + 28 \n",
        "T1 = T2 - 28 # train/test split\n",
        "assert (T2 - T0) % 28 == 0\n",
        "\n",
        "covariates = torch.arange(T2).unsqueeze(-1)\n",
        "snap = m5_data.get_snap().repeat_interleave(torch.tensor([4, 3, 3]), dim=-1)\n",
        "snap = snap.t().unsqueeze(1).unsqueeze(-1)\n",
        "dept = m5_data.get_dummy_dept().reshape(10, -1, 7).unsqueeze(-2)\n",
        "saled = (m5_data.get_prices() != 0).type(torch.get_default_dtype()).reshape(\n",
        "    10, 3049, -1, 1)\n",
        "\n",
        "ma28x1 = data.unfold(-2, 28 * 1, 1).mean(-1)\n",
        "ma28x1 = torch.nn.functional.pad(ma28x1, (0, 0, 27+28*1, 0))\n",
        "ma28x2 = data.unfold(-2, 28 * 2, 1).mean(-1)\n",
        "ma28x2 = torch.nn.functional.pad(ma28x2, (0, 0, 27+28*2, 0))\n",
        "ma28x3 = data.unfold(-2, 28 * 3, 1).mean(-1)\n",
        "ma28x3 = torch.nn.functional.pad(ma28x3, (0, 0, 27+28*3, 0))\n",
        "log_ma = torch.cat([ma28x1, ma28x2, ma28x3], -1).clamp(min=1e-3).log()\n",
        "\n",
        "del ma28x1, ma28x2, ma28x3\n",
        "\n",
        "DEVICE = \"gpu\"\n",
        "data = data.clamp(min=1e-3).to(DEVICE)\n",
        "covariates = covariates.to(DEVICE)\n",
        "snap = snap.to(DEVICE)\n",
        "dept = dept.to(DEVICE)\n",
        "saled = saled.to(DEVICE)\n",
        "log_ma = log_ma.to(DEVICE)\n",
        "\n",
        "if CUDA:\n",
        "  torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-9e6a84e1bf22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcovariates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msnap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm5_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_snap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0msnap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm5_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummy_dept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5UgoK5G3Set",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMm1ckIq1qtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}